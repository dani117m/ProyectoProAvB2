{
  "metadata": {
    "name": "EDA_2",
    "kernelspec": {
		  "display_name": "Scala",
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.types._\nval myDataSchema \u003d StructType(\n    Array(\n        StructField(\"id_persona\", DecimalType(26, 0), true), \n        StructField(\"anio\", IntegerType, true), \n        StructField(\"mes\", IntegerType, true), \n        StructField(\"provincia\", IntegerType, true), \n        StructField(\"canton\", IntegerType, true), \n        StructField(\"area\", StringType, true), \n        StructField(\"genero\", StringType, true), \n        StructField(\"edad\", IntegerType, true), \n        StructField(\"estado_civil\", StringType, true), \n        StructField(\"nivel_de_instruccion\", StringType, true), \n        StructField(\"etnia\", StringType, true), \n        StructField(\"ingreso_laboral\", IntegerType, true), \n        StructField(\"condicion_actividad\", StringType, true), \n        StructField(\"sectorizacion\", StringType, true), \n        StructField(\"grupo_ocupacion\", StringType, true), \n        StructField(\"rama_actividad\", StringType, true), \n        StructField(\"factor_expansion\", DoubleType, true)\n    ));"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val data \u003d spark\n  .read\n  .schema(myDataSchema)\n//  .option(\"inferSchema\", true)\n  .option(\"header\", \"true\")\n  .option(\"delimiter\", \"\\t\")\n  .csv(\"/Users/jorgaf/Documents/Clases/Abril-Agosto2020/Presencial/ProgramacionAvanzada/Proyecto2doBim/Datos_ENEMDU_PEA_v2.csv\");"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "data.schema"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Transformaciones, Acciones y evaluación diferida\nLas operaciones de Spark en un *DataFrame* (datos distribuidos) se pueden clasificar en dos tipos: *transformaciones* y *acciones*.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Transformaciones\nTransforman un DataFrame en un nuevo DataFrame, sin alterar los datos originales ya que estos son inmutables. En pocas palabras, algunas operaciones, tales como *select()* y *filter()* no cambian el DataFrame original, en su lugar, devuelven los resultados de la operación como un nuevo DataFrame.\n\nTodas las *transformaciones* son evaluadas de forma diferida, es decir, sus resultados no se calculan inmediatamente, sino que se registran en un objeto, denominado *lineage*, que es un grafo, que permite a spark optimizar su ejecución, gracias a un plan de ejecución que reorganiza ciertas transformaciones, fusiona u optimiza otras.\n\nLa evaluación diferida es la estrategia de Spark para retrasar la ejecución hasta que se invoque una *acción*. Generalmente una acción devuelve un valor.Col\n\n## Acciones\nUna acción dispara la evaluación diferida de todas las transformaciones\n\n|Transformaciones|Acciones|\n|----------------|--------|\n|orderBy         |show    |\n|groupBy         |take    |\n|filter          |count   |\n|select          |collect |\n|join            |save    |"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Columnas y expresiones\nTal como se mencionó anteriormente un DataFrame de spark posee columnas y filas, que conceptualmente son similares a una tabla de un motor de base de datos.\n\nDentro Apache Spark las columnas de un DataFrame son como objetos que poseen métodos y como tales es posible manipular los valores de las columnas con expresiones o una serie de cálculos matemáticos.\n\nPara escoger una columna se puede usar las palabras *col* o *columna*\n\nEjemplos:\n1. Mostrar todas las columnas\n```scala\ndata.columns\n```\n2. Seleccionar una columna y obtener una objeto columna (Column)\n```scala\ndata.col(\"ingreso_laboral\")\n```\n\n3. Duplicar el ingreso por actividades laborales\n```scala\ndata.select(col(\"ingreso_laboral\") * 2).show(2)\n```\n\nTambién es posible crear expresiones utilizando la sentencia *expr*. Se pueden crear expresiones simples tales como: *expr(\"column_name * 5\")* o *(expr(\"column_name - 5\") \u003e col(\"anothercolumnname\"))*\n\n4. Duplicar el ingreso por las actividades laborales\n```scala\ndata.select(expr(\"ingreso_laboral * 2\")).show(2)\n```\n\nTambién, es posible crear una nueva columna utilizando el resultado de una expresión. Por ejemplo: crear una nueva columna con valores verdadero o falso si el ingreso de una persona es mayor que el salario mínimo vital ($400).\n\n5. Crear una nueva columna con valores verdadero o falso si el ingreso de una persona es mayor que el salario mínimo vital ($400). \n```scala\ndata.withColumn(\"MayorSMV\", (expr(\"ingreso_laboral \u003e 400\"))).show()\n```\n\nUn ejemplo del uso de los métodos de las columnas.\n\n6. Ordenar de forma descendente por el ingreso laboral.\n```\ndata.sort(col(\"ingreso_laboral\").desc).show;\ndata.sort($\"ingreso_laboral\".desc).show;\n```\n\nOtro ejemplo. Cambiar el nombre de una columna.\n\n7. Cambiar el nombre de la columna ingreso_laboral por sueldo.\n```scala\ndata.select($\"ingreso_laboral\".as(\"sueldo\")).show()\n```\n\nHasta aquí, únicamente se ha revisado la superficie de los métodos de las columnas, para mayor detalle se debe revisar: https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Column.html"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Operaciones básicas con DataFrames\n### Proyecciones y filtros\nDentro del argot de las bases de datos relacionales, una proyección es la habilidad de **seleccionar** todas o ciertas columnas y cuyas filas cumplen con una condición que se expresa a través de filtros. En Spark, una proyección se hace usando el método *select()*, mientras que un filtro puede hacerse utilizando los métodos *filter()* o *where()*.\n\nUn ejemplo, seleccionar las columnas etnia, áreas e ingreso laboral en donde la etnia sea diferente de mestizo\n```scala\ndata.select(col(\"etnia\"), col(\"area\"), $\"ingreso_laboral\").where(column(\"etnia\") \u003d!\u003d \"6 - Mestizo\").show\n```\n\nTambién es posible seleccionar los valores únicos de una columna. Por ejemplo, mostrar los valores únicos de la columna *grupo_ocupacion* que sean diferentes de null y ordenarlos alfabéticamente.\n```scala\ndata.select(\"grupo_ocupacion\").distinct.where($\"grupo_ocupacion\" \u003d!\u003d \"null\").sort(\"grupo_ocupacion\").show(false)\n```\nUsando el mismo ejemplo, anterior, ahora se pide contar los valores únicos de la columna *grupo_ocupacion* que sean diferentes de *null*.\n```scala\ndata.select(\"grupo_ocupacion\").distinct.where($\"grupo_ocupacion\".isNotNull).count\n```\n\n### Renombrando, agregando o borrando columnas\nEs posible renombrar columnas por diferentes circunstancias como por ejemplo: estilos o estándares de nombrado. Por ejemplo, la columna *nivel_de_instruccion* se podría cambiar de nombre por uno más orientado a la programación, algo así NivelDeInstruccion.\n```scala\nval data2 \u003d data.withColumnRenamed(\"nivel_de_instruccion\", \"nivelDeInstruccion\")\ndata2.select(\"nivelDeInstruccion\").distinct.show\n```\n\nNo olvide que un DataFrame es inmutable, es decir no puede cambiar un valor, es por ello, que la acción anterior genera una nuevo DataFrame que en este caso se llama *data2* y la segunda sentencia se utiliza para mostrar que en realidad se hizo el cambio.\n\nPara agregar columnas se puede utilzar la sentencia que ya se usó anteriormente, aquí el ejemplo adaptada para el DataFrame *data2*:\n```scala\nval data3 \u003d data2.withColumn(\"MayorSMV\", (expr(\"ingreso_laboral \u003e 400\")))\ndata3.show\n```\nLa anterior sentencia, creará una columna que se llamará *MayorSMV* cuyos valores serán booleanos (true|false) si se cumple la condición de la expresión.\n\nFinalmente, para borrar una columna, se debe usar el método drop y enviar como parámetro el nombre de la columna a eliminar. Para nuestro DataFrame (*data3*), la columna *id_persona* no es necesaria ya que para nuestro propósito (EDA), no tiene información relevante. Para eliminarla se debe hacer lo siguiente:\n```scala\nval data4 \u003d data3.drop(\"id_persona\")\ndata4.schema\n```\n\n### Agregaciones\nAhora trate de responder a la pregunta ¿cuál es la rama de actividad más frecuente en el conjunto de datos? Para buscar su respuesta es necesario usar agregaciones que son métodos que permiten agregar operaciones.\n```scala\ndata4.select(\"rama_actividad\").where($\"rama_actividad\".isNotNull).groupBy(\"rama_actividad\").count().orderBy(desc(\"count\")).show(false)\n```\n\nExisten otras funciones de origen estadístico que se puede implementar y que se estudiarán más adelante, tales como: *min()*, *max()*, *sum()*, *avg()*, etc"
    }
  ]
}